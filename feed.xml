<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://zonca.dev/feed.xml" rel="self" type="application/atom+xml" /><link href="https://zonca.dev/" rel="alternate" type="text/html" /><updated>2020-03-09T13:22:30-05:00</updated><id>https://zonca.dev/feed.xml</id><title type="html">Andrea Zonca</title><subtitle>Tutorials and blog posts by Andrea Zonca: Python, Jupyter, Kubernetes</subtitle><entry><title type="html"></title><link href="https://zonca.dev/2020/03/2013-08-30-interactive-figures-planck-power-spectra.html" rel="alternate" type="text/html" title="" /><published>2020-03-09T13:22:30-05:00</published><updated>2020-03-09T13:22:30-05:00</updated><id>https://zonca.dev/2020/03/2013-08-30-interactive-figures-planck-power-spectra</id><content type="html" xml:base="https://zonca.dev/2020/03/2013-08-30-interactive-figures-planck-power-spectra.html">&lt;p&gt;
 For a long time I've been curious about trying out
 &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;
  d3.js
 &lt;/span&gt;
 , the javascript plotting library which is becoming the standard for interactive plotting in the browser.
 &lt;br /&gt;
&lt;/p&gt;
&lt;div&gt;
 &lt;br /&gt;
&lt;/div&gt;
&lt;div&gt;
 What is really appealing is the capability of sharing with other people powerful interactive visualization simply via the link to a web page. This will hopefully be the future of scientific publications, as envisioned, for example, by
 &lt;a href=&quot;https://www.authorea.com/&quot;&gt;
  Authorea
 &lt;/a&gt;
 .
&lt;/div&gt;
&lt;div&gt;
 &lt;a name=&quot;more&quot;&gt;
 &lt;/a&gt;
 An interesting example related to my work on Planck is a plot of the high number of Angular Power Spectra of the anisotropies of the Cosmic Microwave Background Temperature.
&lt;/div&gt;
&lt;div&gt;
 The CMB Power spectra describe how the temperature fluctuations were distributed in the sky as a function of the angular scale, for example the largest peak at about 1 degree means that the brightest cold/warm spots of the CMB have that angular size, see
 &lt;a href=&quot;http://www.strudel.org.uk/blog/astro/001030.shtml&quot;&gt;
  The Universe Simulator in the browser
 &lt;/a&gt;
 .
&lt;/div&gt;
&lt;div&gt;
 The
 &lt;a href=&quot;http://irsa.ipac.caltech.edu/data/Planck/release_1/ancillary-data/&quot;&gt;
  Planck Collaboration released
 &lt;/a&gt;
 a combined spectrum, which aggregates several channels to give the best result, spectra frequency by frequency (for some frequencies split in detector-sets) and a best-fit spectrum given a Universe Model.
&lt;/div&gt;
&lt;div&gt;
 It is also interesting to compare to the latest release spectrum by WMAP with 9 years of data.
&lt;/div&gt;
&lt;div&gt;
 &lt;br /&gt;
&lt;/div&gt;
&lt;div&gt;
 The plan is to create a visualization where it is easier to zoom to different angular scales on the horizontal axis and quickly show/hide each curve.
&lt;/div&gt;
&lt;div&gt;
 For this I used
 &lt;a href=&quot;http://code.shutterstock.com/rickshaw/&quot;&gt;
  rickshaw
 &lt;/a&gt;
 , a library based on
 &lt;span style=&quot;font-family: Courier New, Courier, monospace;&quot;&gt;
  d3.js
 &lt;/span&gt;
 &lt;span style=&quot;font-family: inherit;&quot;&gt;
  which makes it easier to create time-series plots.
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style=&quot;font-family: inherit;&quot;&gt;
  In fact most of the features are already implemented, it is just a matter of configuring them, see the code on github:
 &lt;/span&gt;
 &lt;a href=&quot;https://github.com/zonca/visualize-planck-cl&quot;&gt;
  https://github.com/zonca/visualize-planck-cl
 &lt;/a&gt;
&lt;/div&gt;
&lt;div&gt;
 The most complex task is actually to load all the data, previously converted to JSON, in the background from the server and push them in a data structure which is understood by rickshaw.
&lt;/div&gt;
&lt;div&gt;
 &lt;br /&gt;
&lt;/div&gt;
&lt;div&gt;
 Check out the result:
&lt;/div&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;
 &lt;b&gt;
  &lt;a href=&quot;http://bit.ly/planck-spectra&quot;&gt;
   http://bit.ly/planck-spectra
  &lt;/a&gt;
 &lt;/b&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;br /&gt;
&lt;/div&gt;</content><author><name></name></author></entry><entry><title type="html"></title><link href="https://zonca.dev/2020/03/2014-02-13-openproceedings.html" rel="alternate" type="text/html" title="" /><published>2020-03-09T13:22:30-05:00</published><updated>2020-03-09T13:22:30-05:00</updated><id>https://zonca.dev/2020/03/2014-02-13-openproceedings</id><content type="html" xml:base="https://zonca.dev/2020/03/2014-02-13-openproceedings.html">&lt;p&gt;Github provides a great interface for gathering, peer reviewing and accepting papers for conference proceedings, the second step is to publish them on a website either in HTML or PDF form or both.
The Scipy conference is at the forefront on this and did great work in peer reviewing on Github, see: &lt;a href=&quot;https://github.com/scipy-conference/scipy_proceedings/pull/61&quot;&gt;https://github.com/scipy-conference/scipy_proceedings/pull/61&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I wanted to develop a system to make it easier to continously publish updated versions of the papers and also leverage FigShare to provide a long term repository, a sharing interface and a &lt;a href=&quot;http://en.wikipedia.org/wiki/Digital_object_identifier&quot;&gt;DOI&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I based it on the blog engine &lt;a href=&quot;http://getpelican.com&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Pelican&lt;/code&gt;&lt;/a&gt;, developed a plugin &lt;a href=&quot;http://github.com/openproceedings/pelican_figshare_pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;figshare_pdf&lt;/code&gt;&lt;/a&gt; to upload a PDF of an article via API and configured &lt;a href=&quot;http://travis-ci.org&quot;&gt;Travis-ci&lt;/a&gt; as building platform.&lt;/p&gt;

&lt;p&gt;See more details on the project page on Github:
&lt;a href=&quot;https://github.com/openproceedings/openproceedings-buildbot&quot;&gt;https://github.com/openproceedings/openproceedings-buildbot&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author></entry><entry><title type="html"></title><link href="https://zonca.dev/2020/03/2008-04-29-netcat-quickly-send-binaries-through.html" rel="alternate" type="text/html" title="" /><published>2020-03-09T13:22:30-05:00</published><updated>2020-03-09T13:22:30-05:00</updated><id>https://zonca.dev/2020/03/2008-04-29-netcat-quickly-send-binaries-through</id><content type="html" xml:base="https://zonca.dev/2020/03/2008-04-29-netcat-quickly-send-binaries-through.html">&lt;p&gt;
 just start nc in server mode on localhost:
 &lt;br /&gt;
 &lt;br /&gt;
 [sourcecode language='python'] nc -l -p 3333 [/sourcecode]
 &lt;br /&gt;
 &lt;br /&gt;
 send a string to localhost on port 3333:
 &lt;br /&gt;
 &lt;br /&gt;
 [sourcecode language='python'] echo &quot;hello world&quot; | nc localhost 3333 [/sourcecode]
 &lt;br /&gt;
 &lt;br /&gt;
 you'll see on server side appearing the string you sent.
 &lt;br /&gt;
 &lt;br /&gt;
 very useful for sending binaries, see
 &lt;a href=&quot;http://www.g-loaded.eu/2006/11/06/netcat-a-couple-of-useful-examples/&quot;&gt;
  examples
 &lt;/a&gt;
 .
&lt;/p&gt;</content><author><name></name></author></entry><entry><title type="html">Migrate from Pelican to Fastpages</title><link href="https://zonca.dev/2020/03/pelican-to-fastpages.html" rel="alternate" type="text/html" title="Migrate from Pelican to Fastpages" /><published>2020-03-09T00:00:00-05:00</published><updated>2020-03-09T00:00:00-05:00</updated><id>https://zonca.dev/2020/03/pelican-to-fastpages</id><content type="html" xml:base="https://zonca.dev/2020/03/pelican-to-fastpages.html">&lt;p&gt;I have been using the Pelican static website generator for a few years,
hosting the content on Github, automatically build on push via Travis-CI
and deploy on Github pages to &lt;code class=&quot;highlighter-rouge&quot;&gt;zonca.github.io&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I am a heavy Jupyter Notebook user so once I saw the announcement of &lt;a href=&quot;https://fastpages.fast.ai/&quot;&gt;Fastpages&lt;/a&gt;
I decided it was time to switch.
I loved the idea of having Jupyter Notebooks built-in and not added via plugins,
also great idea to use Github actions.&lt;/p&gt;

&lt;p&gt;Only issue I found was that you cannot setup Fastpages on &lt;code class=&quot;highlighter-rouge&quot;&gt;username.github.io&lt;/code&gt;,
so went for using a custom domain name instead.&lt;/p&gt;

&lt;h2 id=&quot;import-content&quot;&gt;Import content&lt;/h2&gt;

&lt;p&gt;I created a script, in Python of course, to modify the front matter of the markdown
posts from the Pelican formatting to Jekyll, see &lt;a href=&quot;https://gist.github.com/zonca/b4a5a44513854e1c8918743d219f5f34&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pelican_to_jekyll.py&lt;/code&gt;&lt;/a&gt;.
It also renames the files, because Jekyll expects a date at the beginning of filenames.&lt;/p&gt;

&lt;h2 id=&quot;setup-paginate&quot;&gt;Setup paginate&lt;/h2&gt;

&lt;p&gt;Currently Fastpages doesn’t support pagination for the homepage,
but &lt;a href=&quot;https://github.com/fastai/fastpages/issues/48#issuecomment-596608688&quot;&gt;implemented a workaround&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;redirect-from-the-old-github-pages-blog&quot;&gt;Redirect from the old Github Pages blog&lt;/h2&gt;

&lt;p&gt;I modified the permalinks of Fastpages so that I have the same URLs in the old and new websites,
just the domain changes.
Github pages does not support custom rewriting rules, so I modified the Pelican template
to put a custom redirection tag in each HTML header.&lt;/p&gt;

&lt;p&gt;In the Pelican template &lt;code class=&quot;highlighter-rouge&quot;&gt;article.html&lt;/code&gt;, in the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;header&amp;gt;&lt;/code&gt; section I added:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=https://zonca.dev/&quot;&amp;gt;
&amp;lt;link rel=&quot;canonical&quot; href=&quot;https://zonca.dev/&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So that Pelican regenerated all the articles with their original address
and automatically redirects upon access.
The canonical link hopefully helps with SEO.&lt;/p&gt;

&lt;h2 id=&quot;screenshots-of-the-old-blog&quot;&gt;Screenshots of the old blog&lt;/h2&gt;

&lt;p&gt;Yeah, for posterity, growing older I get more nostalgic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/old_blog_homepage.png&quot; alt=&quot;Old blog homepage&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/old_blog_article_page.png&quot; alt=&quot;Old blog article page&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">I have been using the Pelican static website generator for a few years, hosting the content on Github, automatically build on push via Travis-CI and deploy on Github pages to zonca.github.io.</summary></entry><entry><title type="html">Deploy CVMFS on Kubernetes</title><link href="https://zonca.dev/2020/02/cvmfs-kubernetes.html" rel="alternate" type="text/html" title="Deploy CVMFS on Kubernetes" /><published>2020-02-26T13:00:00-06:00</published><updated>2020-02-26T13:00:00-06:00</updated><id>https://zonca.dev/2020/02/cvmfs-kubernetes</id><content type="html" xml:base="https://zonca.dev/2020/02/cvmfs-kubernetes.html">&lt;p&gt;&lt;a href=&quot;https://cvmfs.readthedocs.io/&quot;&gt;CVMFS&lt;/a&gt; is a software distribution service, it is used by High Energy Physics experiments at CERN
to synchronize software environments across the whole collaborations.&lt;/p&gt;

&lt;p&gt;In the context of a Kubernetes + JupyterHub deployment on Jetstream, for example &lt;a href=&quot;http://zonca.github.io/2019/06/kubernetes-jupyterhub-jetstream-magnum.html&quot;&gt;deployed using Magnum following my tutorial&lt;/a&gt;, it is useful to use CVMFS to make the software tools of a collaboration to all the users connected to JupyterHub, so that we can keep the base Docker image simpler and smaller.&lt;/p&gt;

&lt;h2 id=&quot;alternatives&quot;&gt;Alternatives&lt;/h2&gt;

&lt;p&gt;A already existing solution is &lt;a href=&quot;https://github.com/cernops/cvmfs-csi&quot;&gt;the CVMFS CSI driver&lt;/a&gt;, however it doesn’t have much documentation, so I haven’t tested it. It would be useful for larger deployments, but we are designing for a 5 (possibly up to 10) nodes Kubernetes cluster.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;We have a pod running in Kubernetes (running as a privileged Docker container) which runs the CVMFS client and caches locally
(on a dedicated Openstack volume) some pre-defined CVMFS repositories (at the moment we do not support automounting).&lt;/p&gt;

&lt;p&gt;Currently we are using the &lt;code class=&quot;highlighter-rouge&quot;&gt;DIRECT&lt;/code&gt; connection for the CVMFS client, due to having just a single client which accesses
a small amount of data. Using a proxy is required instead for heavier usage, and it could also be deployed inside Kubernetes.&lt;/p&gt;

&lt;p&gt;The same pod also runs a NFS server and exposes it internally into the Kubernetes cluster, over the local Jetstream network,
to any other pod which can use a NFS volume and mount it to the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; folder inside the container.
We also activate the CVMFS configuration options for NFS support, following the &lt;a href=&quot;https://cvmfs.readthedocs.io/en/stable/cpt-configure.html#nfs-server-mode&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;The repositories used in this deployment are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zonca/docker-cvmfs-client&quot;&gt;Github repository for the Docker image of the CVMFS client&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Docker Hub repositories where the 2 containers are built: &lt;a href=&quot;https://hub.docker.com/r/zonca/cvmfs-client&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cvmfs-client&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://hub.docker.com/r/zonca/cvmfs-client-nfs&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cvmfs-client-nfs&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/tree/master/cvmfs&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jupyterhub-deploy-kubernetes-jetstream&lt;/code&gt;&lt;/a&gt; Github repositories with the Kubernetes configuration files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First we need to checkout the &lt;code class=&quot;highlighter-rouge&quot;&gt;jupyterhub-deploy-kubernetes-jetstream&lt;/code&gt; repository:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream.git
cd jupyterhub-deploy-kubernetes-jetstream/cvmfs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then configure the CVMFS pod with the required repositories, see the &lt;code class=&quot;highlighter-rouge&quot;&gt;CVMFS_REPOSITORIES&lt;/code&gt; variable in &lt;a href=&quot;https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/blob/master/cvmfs/pod_cvmfs_nfs.yaml&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pod_cvmfs_nfs.yaml&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then deploy the pod with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f pod_cvmfs_nfs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This creates 2 Openstack volumes, a 20 GB volume for the CVMFS cache, and a 1 GB volume which is just necessary as the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; root folder of the NFS server.
It also creates the &lt;code class=&quot;highlighter-rouge&quot;&gt;nfs-service&lt;/code&gt; Service, with a fixed IP, so that we can use it in the pod using this.&lt;/p&gt;

&lt;p&gt;Finally we can create a pod using mounting the folder via NFS:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f test_nfs_mount.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then get a terminal in the pod with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash ../terminal_pod.sh test-nfs-mount
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This creates a volume which mounts the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; folder shared with NFS, this automatically also shares also all the subfolders.&lt;/p&gt;

&lt;p&gt;Finally we can check the content of the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; folder.&lt;/p&gt;</content><author><name></name></author><summary type="html">CVMFS is a software distribution service, it is used by High Energy Physics experiments at CERN to synchronize software environments across the whole collaborations.</summary></entry><entry><title type="html">Organize calendars for a large scientific collaboration</title><link href="https://zonca.dev/2019/12/organize-calendar-collaboration.html" rel="alternate" type="text/html" title="Organize calendars for a large scientific collaboration" /><published>2019-12-02T12:00:00-06:00</published><updated>2019-12-02T12:00:00-06:00</updated><id>https://zonca.dev/2019/12/organize-calendar-collaboration</id><content type="html" xml:base="https://zonca.dev/2019/12/organize-calendar-collaboration.html">&lt;p&gt;Many scientific collaborations have a central calendar, often hosted on Google Calendar,
to coordinate Teleconferences, meetings and events across timezones.&lt;/p&gt;

&lt;h3 id=&quot;the-issue&quot;&gt;The issue&lt;/h3&gt;

&lt;p&gt;Most users are only interested in a small subset of the events, however Google Calendar
does not allow them to subscribe to single events. The central calendar admin could invite
each person to events, but that requires lots of work.&lt;/p&gt;

&lt;p&gt;So, users either subscribe to the whole calendar, but then have a huge clutter of un-interesting events,
or copy just a subset of the events to their calendars, but loose track of any rescheduling of the
original event.&lt;/p&gt;

&lt;h3 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h3&gt;

&lt;p&gt;I recommend to split the events across multiple calendars, for example one for each working group,
or any other categorization where most users would be interested in all events in a calendar.
And possibly a “General” category with events that should interest the whole collaboration.&lt;/p&gt;

&lt;p&gt;Still, we can embed all of the calendars in a single webpage, see an example below where 2 calendars (Monday and Tuesday telecon calendars) are visualized together, &lt;a href=&quot;https://support.google.com/calendar/answer/41207?hl=en&quot;&gt;see the Google Calendar documentation&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;https://calendar.google.com/calendar/embed?height=600&amp;amp;wkst=1&amp;amp;bgcolor=%23ffffff&amp;amp;ctz=America%2FLos_Angeles&amp;amp;src=dTI2dnBkNnZvcm1qNHVucnVtajMzZzdwcGNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&amp;amp;src=c2FwazM1OTVmcHRiZHVtOWdqZnJwdWxkbnNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&amp;amp;color=%23DD4477&amp;amp;color=%236633CC&quot; style=&quot;border-width:0&quot; width=&quot;800&quot; height=&quot;600&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Users can click on the bottom “Add to Google Calendar” button and subscribe to a subset or all the calendars.
See the screenshot below, &lt;img src=&quot;/images/add_google_calendar.png&quot; alt=&quot;screenshot of add to Google Calendar&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;As an additional benefit, we can compartimentalize permissions more easily, e.g. leads of a working group
get writing access only to their relevant calendar/calendars.&lt;/p&gt;</content><author><name></name></author><summary type="html">Many scientific collaborations have a central calendar, often hosted on Google Calendar, to coordinate Teleconferences, meetings and events across timezones.</summary></entry><entry><title type="html">Simulate users on JupyterHub</title><link href="https://zonca.dev/2019/10/loadtest-jupyterhub.html" rel="alternate" type="text/html" title="Simulate users on JupyterHub" /><published>2019-10-30T12:00:00-05:00</published><updated>2019-10-30T12:00:00-05:00</updated><id>https://zonca.dev/2019/10/loadtest-jupyterhub</id><content type="html" xml:base="https://zonca.dev/2019/10/loadtest-jupyterhub.html">&lt;p&gt;I currently have 2 different strategies to deploy JupyterHub on top of Kubernetes on Jetstream:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using &lt;a href=&quot;https://zonca.github.io/2019/02/kubernetes-jupyterhub-jetstream-kubespray.html&quot;&gt;Kubespray&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Using &lt;a href=&quot;http://zonca.github.io/2019/06/kubernetes-jupyterhub-jetstream-magnum.html&quot;&gt;Magnum&lt;/a&gt;, which also supports the &lt;a href=&quot;http://zonca.github.io/2019/09/kubernetes-jetstream-autoscaler.html&quot;&gt;Cluster Autoscaler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this tutorial I’ll show how to use Yuvi Pandas’ &lt;a href=&quot;https://github.com/yuvipanda/hubtraf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt;&lt;/a&gt; to simulate load on JupyterHub, i.e. programmatically generate a predefined number of users connecting and executing notebooks on the system.&lt;/p&gt;

&lt;p&gt;This is especially useful to test the Cluster Autoscaler.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; assumes you are using the Dummy authenticator, which is the default installed by the &lt;code class=&quot;highlighter-rouge&quot;&gt;zero-to-jupyterhub&lt;/code&gt; helm chart. If you have configured another authenticator, temporarily disable it for testing purposes.&lt;/p&gt;

&lt;p&gt;First go through the &lt;a href=&quot;https://github.com/yuvipanda/hubtraf/blob/master/docs/index.rst#jupyterhub-traffic-simulator&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; documentation&lt;/a&gt; to understand its functionalities.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; also has a Helm recipe to run it within Kubernetes, but the simpler way is to test from your laptop, follow the [documentation of &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt;] to install the package and then run:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hubtraf http://js-xxx-yyy.jetstream-cloud.org 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To simulate 2 users connecting to the system, you can then check with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods -n jhub
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That the pods are being created successfully and check the logs on the command line from &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; which explains what it is doing and tracks the time every operation takes, so it is useful to debug any delay in providing resources to users.&lt;/p&gt;

&lt;p&gt;Consider that volumes created by JupyterHub for the test users will remain in Kubernetes and in Openstack, therefore if you would like to use the same deployment for production, remember to cleanup the Kubernetes &lt;code class=&quot;highlighter-rouge&quot;&gt;PersistentVolume&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;PersistentVolumeClaim&lt;/code&gt; resources.&lt;/p&gt;

&lt;p&gt;Now we can test scalability of the deployment with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    hubtraf http://js-xxx-yyy.jetstream-cloud.org 100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Make sure you have asked the XSEDE support to increase the maximum number of volumes in Openstack in your allocation that by default is only 10. Otherwise edit &lt;code class=&quot;highlighter-rouge&quot;&gt;config_standard_storage.yaml&lt;/code&gt; and set:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;singleuser:
  storage:
    type: none
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;test-the-cluster-autoscaler&quot;&gt;Test the Cluster Autoscaler&lt;/h2&gt;

&lt;p&gt;If you followed the tutorial to deploy the Cluster Autoscaler on Magnum, you can launch &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; to create a large number of pods, then check that some pods are “Running” and the ones that do not fit in the current nodes are “Pending”:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods -n jhub
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then check in the logs of the autoscaler that it detects that those pods are pending and requests additional nodes.
For example:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; kubectl logs &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system cluster-autoscaler-hhhhhhh-uuuuuuu
I1031 00:48:39.807384       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2
I1031 00:48:41.583449       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&amp;gt;2
I1031 00:49:14.141351       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After 4 or 5 minutes the new node should be available and should show up in:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And we can check that some user pods are now running on the new node:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods -n jhub -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In my case the Autoscaler actually requested a 3rd node to accomodate all the users pods:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I1031 00:48:39.807384       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2
I1031 00:48:41.583449       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&amp;gt;2
I1031 00:49:14.141351       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
I1031 00:52:51.308054       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
I1031 00:53:01.315179       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 3
I1031 00:53:02.996583       1 magnum_nodegroup.go:101] Increasing size by 1, 2-&amp;gt;3
I1031 00:53:35.607158       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
I1031 00:56:41.834151       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Moreover Cluster Autoscaler also provides useful information in the status of each “Pending” node. For example if it detects that it is useless to create a new node because the node is “Pending” for some other reason (e.g. volume quota reached), this infomation will be accessible using:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl describe node -n jhub jupyter-xxxxxxx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the simulated users disconnect, &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; has a default of about 5 minutes, the autoscaler waits for the configured amount of minutes, by default it is 10 minutes, in my deployment it is 1 minute to simplify testing, see the &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-deployment-master.yaml&lt;/code&gt; file.
After this delay, the autoscaler scales down the size of the cluster, it is a 2 step process, it first terminates the Openstack Virtual machine and then adjusts the size of the Magnum cluster (&lt;code class=&quot;highlighter-rouge&quot;&gt;node_count&lt;/code&gt;), you can monitor the process using &lt;code class=&quot;highlighter-rouge&quot;&gt;openstack server list&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;openstack coe cluster list&lt;/code&gt;, and the log of the autoscaler:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I1101 06:31:10.223660       1 scale_down.go:882] Scale-down: removing empty node k8s-e2iw7axmhym7-minion-1 
I1101 06:31:16.081223       1 magnum_manager_heat.go:276] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;stack UPDATE_IN_PROGRESS status
I1101 06:32:17.061860       1 magnum_manager_heat.go:276] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;stack UPDATE_COMPLETE status
I1101 06:32:49.826439       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
I1101 06:33:21.588022       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;Thanks Yuvi Panda for providing &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt;, thanks Julien Chastang for testing my deployments.&lt;/p&gt;</content><author><name></name></author><summary type="html">I currently have 2 different strategies to deploy JupyterHub on top of Kubernetes on Jetstream:</summary></entry><entry><title type="html">Execute Jupyter Notebooks not interactively</title><link href="https://zonca.dev/2019/09/batch-notebook-execution.html" rel="alternate" type="text/html" title="Execute Jupyter Notebooks not interactively" /><published>2019-09-23T12:00:00-05:00</published><updated>2019-09-23T12:00:00-05:00</updated><id>https://zonca.dev/2019/09/batch-notebook-execution</id><content type="html" xml:base="https://zonca.dev/2019/09/batch-notebook-execution.html">&lt;p&gt;Over the years, I have explored how to scale up easily computation through
Jupyter Notebooks by executing them not-interactively, possibily parametrized
and remotely. This is mostly for reference.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zonca/nbsubmit&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nbsubmit&lt;/code&gt;&lt;/a&gt; is a Python package which has Python API to send a local notebook for execution on a remote SLURM cluster, for example Comet, see &lt;a href=&quot;https://github.com/zonca/nbsubmit/blob/master/example/multiple_jobs/submit_multiple_jobs.ipynb&quot;&gt;an example&lt;/a&gt;. This project is not maintained right now.&lt;/li&gt;
  &lt;li&gt;Back in 2017 I tested submitting notebooks to Open Science Grid, see &lt;a href=&quot;https://github.com/zonca/batch-notebooks-condor&quot;&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;batch-notebooks-condor&lt;/code&gt; repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Back in 2016 I created scripts to template a Jupyter Notebook and launch SLURM jobs, see &lt;a href=&quot;https://github.com/sdsc/sdsc-summer-institute-2016/blob/master/hpc3_python_hpc/slurm.shared.template&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slurm.shared.template&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/sdsc/sdsc-summer-institute-2016/blob/master/hpc3_python_hpc/runipyloop.sh&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;runipyloop.sh&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Over the years, I have explored how to scale up easily computation through Jupyter Notebooks by executing them not-interactively, possibily parametrized and remotely. This is mostly for reference.</summary></entry><entry><title type="html">Deploy Cluster Autoscaler for Kubernetes on Jetstream</title><link href="https://zonca.dev/2019/09/kubernetes-jetstream-autoscaler.html" rel="alternate" type="text/html" title="Deploy Cluster Autoscaler for Kubernetes on Jetstream" /><published>2019-09-12T12:00:00-05:00</published><updated>2019-09-12T12:00:00-05:00</updated><id>https://zonca.dev/2019/09/kubernetes-jetstream-autoscaler</id><content type="html" xml:base="https://zonca.dev/2019/09/kubernetes-jetstream-autoscaler.html">&lt;p&gt;The &lt;a href=&quot;https://github.com/kubernetes/autoscaler&quot;&gt;Kubernetes Cluster Autoscaler&lt;/a&gt; is a service
that runs within a Kubernetes cluster and when there are not enough resources to accomodate
the pods that are queued to run, it contacts the API of the cloud provider to create
more Virtual Machines to join the Kubernetes Cluster.&lt;/p&gt;

&lt;p&gt;Initially the Cluster Autoscaler only supported commercial cloud provides, but back in
March 2019 &lt;a href=&quot;https://github.com/kubernetes/autoscaler/pull/1690&quot;&gt;a user contributed Openstack support based on Magnum&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First step you should have a Magnum-based deployment running on Jetstream,
see &lt;a href=&quot;https://zonca.github.io/2019/06/kubernetes-jupyterhub-jetstream-magnum.html&quot;&gt;my recent tutorial about that&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Therefore you should also have already a copy of the repository of all configuration
files checked out on your local machine that you are using to interact with the openstack API,
if not:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and enter the folder dedicated to the autoscaler:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd jupyterhub-deploy-kubernetes-jetstream/kubernetes_magnum/autoscaler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;setup-credentials&quot;&gt;Setup credentials&lt;/h2&gt;

&lt;p&gt;We first create the service account needed by the autoscaler to interact with the Kubernetes API:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cluster-autoscaler-svcaccount.yaml 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we need to provide all connection details for the autoscaler to interact with the Openstack API,
those are contained in the &lt;code class=&quot;highlighter-rouge&quot;&gt;cloud-config&lt;/code&gt; of our cluster available in the master node and setup
by Magnum.
Get the &lt;code class=&quot;highlighter-rouge&quot;&gt;IP&lt;/code&gt; of your master node from:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack server list
&lt;span class=&quot;nv&quot;&gt;IP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xxx.xxx.xxx.xxx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now ssh into the master node and access the &lt;code class=&quot;highlighter-rouge&quot;&gt;cloud-config&lt;/code&gt; file:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh fedora@&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/kubernetes/cloud-config 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;now copy the &lt;code class=&quot;highlighter-rouge&quot;&gt;[Global]&lt;/code&gt; section at the end of &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-secret.yaml&lt;/code&gt; on the local machine.
Also remove the line of &lt;code class=&quot;highlighter-rouge&quot;&gt;ca-file&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cluster-autoscaler-secret.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;launch-the-autoscaler-deployment&quot;&gt;Launch the Autoscaler deployment&lt;/h2&gt;

&lt;p&gt;Create the Autoscaler deployment:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cluster-autoscaler-deployment-master.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Alternatively, I also added a version for a cluster where we are not deploying pods on master &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-deployment.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Check that the deployment is active:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system get pods
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
cluster-autoscaler     1         1         1            0           10s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And check its logs:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system logs cluster-autoscaler-59f4cf4f4-4k4p2

I0905 05:29:21.589062       1 leaderelection.go:217] attempting to acquire leader lease  kube-system/cluster-autoscaler...
I0905 05:29:39.412449       1 leaderelection.go:227] successfully acquired lease kube-system/cluster-autoscaler
I0905 05:29:43.896557       1 magnum_manager_heat.go:293] For stack ID 17ab3ae7-1a81-43e6-98ec-b6ffd04f91d3, stack name is k8s-lu3bksbwsln3
I0905 05:29:44.146319       1 magnum_manager_heat.go:310] Found nested kube_minions stack: name k8s-lu3bksbwsln3-kube_minions-r4lhlv5xuwu3, ID d0590824-cc70-4da5-b9ff-8581d99c666b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you redeploy the cluster and keep a older authentication, you’ll see “Authentication failed” in the logs of the autoscaler pod, you need to update the secret every time you redeploy the cluster.&lt;/p&gt;

&lt;h2 id=&quot;test-the-autoscaler&quot;&gt;Test the autoscaler&lt;/h2&gt;

&lt;p&gt;Now we need to produce a significant load on the cluster so that the autoscaler is triggered to request Openstack Magnum to create more Virtual Machines.&lt;/p&gt;

&lt;p&gt;We can create a deployment of the NGINX container (any other would work for this test):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create deployment autoscaler-demo &lt;span class=&quot;nt&quot;&gt;--image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then create a large number of replicas:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl scale deployment autoscaler-demo &lt;span class=&quot;nt&quot;&gt;--replicas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;300
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are using 2 nodes with a large amount of memory and CPU, so they can accommodate more then 200 of those pods. The rest remains in the queue:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get deployment autoscaler-demo
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
autoscaler-demo   300       300       300          213         18m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And this triggers the autoscaler:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system logs cluster-autoscaler-59f4cf4f4-4k4p2

I0905 05:34:47.401149       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2
I0905 05:34:49.267280       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&amp;gt;2
I0905 05:35:22.222387       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check also in the Openstack API:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack coe cluster list
+------+------+---------+------------+--------------+--------------------+
| uuid | name | keypair | node_count | master_count | status             |
+------+------+---------+------------+--------------+--------------------+
| 09fcf| k8s  | comet   |          2 |            1 | UPDATE_IN_PROGRESS |
+------+------+---------+------------+--------------+--------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It takes about 4 minutes for a new VM to boot, be configured by Magnum and join the Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Checking the logs again should show another line:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I0912 17:18:28.290987       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then you should have all 3 nodes available:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
NAME                        STATUS   ROLES    AGE   VERSION
k8s-6bawhy45wr5t-master-0   Ready    master   38m   v1.11.1
k8s-6bawhy45wr5t-minion-0   Ready    &amp;lt;none&amp;gt;   38m   v1.11.1
k8s-6bawhy45wr5t-minion-1   Ready    &amp;lt;none&amp;gt;   30m   v1.11.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and all 300 NGINX containers deployed:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get deployments
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
autoscaler-demo   300       300       300          300         35m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also test scaling down by scaling back the number of NGINX containers to only a few and check in the logs
of the autoscaler that this process triggers the scale-down process.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-deployment-master.yaml&lt;/code&gt; I have configured the scale down process to trigger just after 1 minute, to simplify testing. For production, better increase this to 10 minutes or more. Check the &lt;a href=&quot;https://github.com/zonca/autoscaler/blob/cluster-autoscaler-1.14-magnum/cluster-autoscaler/FAQ.md&quot;&gt;documentation of Cluster Autoscaler 1.14&lt;/a&gt; for all other available options.&lt;/p&gt;

&lt;h2 id=&quot;note-about-the-cluster-autoscaler-container&quot;&gt;Note about the Cluster Autoscaler container&lt;/h2&gt;

&lt;p&gt;The Magnum provider was added in Cluster Autoscaler 1.15, however this version is not compatible with Kubernetes 1.11 which is currently available on Jetstream. Therefore I have taken the development version of Cluster Autoscaler 1.14 and compiled it myself. I also noticed that the scale down process was not working due to incompatible IDs when the Cloud Provider tried to lookup the ID of a Minion in the Stack. I am now directly using the MachineID instead of going through these indices. This version is available in &lt;a href=&quot;https://github.com/zonca/autoscaler/tree/cluster-autoscaler-1.14-magnum&quot;&gt;my fork of &lt;code class=&quot;highlighter-rouge&quot;&gt;autoscaler&lt;/code&gt;&lt;/a&gt; and it is built into docker containers on the &lt;a href=&quot;https://cloud.docker.com/repository/docker/zonca/k8s-cluster-autoscaler-jetstream&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;zonca/k8s-cluster-autoscaler-jetstream&lt;/code&gt; repository on Docker Hub&lt;/a&gt;.
The image tags are the short version of the repository git commit hash.&lt;/p&gt;

&lt;p&gt;I build the container using the &lt;code class=&quot;highlighter-rouge&quot;&gt;run_gobuilder.sh&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;run_build_autoscaler_container.sh&lt;/code&gt; scripts included in the repository.&lt;/p&gt;

&lt;h2 id=&quot;note-about-images-used-by-magnum&quot;&gt;Note about images used by Magnum&lt;/h2&gt;

&lt;p&gt;I have tested this deployment using the &lt;code class=&quot;highlighter-rouge&quot;&gt;Fedora-Atomic-27-20180419&lt;/code&gt; image on Jetstream at Indiana University.
The Fedora Atomic 28 image had a long hang-up during boot and took more than 10 minutes to start and that caused timeout in the autoscaler and anyway it would have been too long for a user waiting to start a notebook.&lt;/p&gt;

&lt;p&gt;I also tried updating the Fedora Atomic 28 image with &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo atomic host upgrade&lt;/code&gt; and while this fixed the slow startup issue, it generated a broken Kubernetes installation, i.e. the Kubernetes services didn’t detect the master node as part of the cluster, &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get nodes&lt;/code&gt; only showed the minion.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Kubernetes Cluster Autoscaler is a service that runs within a Kubernetes cluster and when there are not enough resources to accomodate the pods that are queued to run, it contacts the API of the cloud provider to create more Virtual Machines to join the Kubernetes Cluster.</summary></entry><entry><title type="html">Create a Github account for your research group with free private repositories</title><link href="https://zonca.dev/2019/08/github-for-research-groups.html" rel="alternate" type="text/html" title="Create a Github account for your research group with free private repositories" /><published>2019-08-24T15:00:00-05:00</published><updated>2019-08-24T15:00:00-05:00</updated><id>https://zonca.dev/2019/08/github-for-research-groups</id><content type="html" xml:base="https://zonca.dev/2019/08/github-for-research-groups.html">&lt;p&gt;&lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt; allows a research group to create their own webpage where they can host, share and develop their software using the &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; version control system and the powerful Github online issue-tracking interface.&lt;/p&gt;

&lt;p&gt;Github offers unlimited private and public repositories to research groups and classrooms.
Private repositories are useful for early stages of development or if it is necessary to keep software secret before publication, at publication they can easily switched to public repositories and free up their slot.&lt;/p&gt;

&lt;p&gt;They also provide free data packs for &lt;a href=&quot;https://git-lfs.github.com/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt;(Large File Support)&lt;/a&gt; which is useful to store large amount of binary data together with your software in the same repository, without actually committing the files into &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; but using a support server. Just go into “Settings” for your organization and under “Billing” add data packs, you will notice that the cost is $0.&lt;/p&gt;

&lt;p&gt;Here the steps to set this up:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a user account on Github and choose the free plan, use your &lt;code class=&quot;highlighter-rouge&quot;&gt;.edu&lt;/code&gt; email address&lt;/li&gt;
  &lt;li&gt;Create an organization account for your research group&lt;/li&gt;
  &lt;li&gt;Go to &lt;a href=&quot;https://education.github.com/&quot;&gt;https://education.github.com/&lt;/a&gt; and click on “Get benefits”&lt;/li&gt;
  &lt;li&gt;Choose what is your position, e.g. Researcher and select you want a discount for an organization&lt;/li&gt;
  &lt;li&gt;Choose the organization you created earlier and confirm that it is a “Research group”&lt;/li&gt;
  &lt;li&gt;Add details about your Research group&lt;/li&gt;
  &lt;li&gt;Finally you need to upload a picture of your University ID card and write how you plan on using the repositories&lt;/li&gt;
  &lt;li&gt;Within a week at most, but generally in less than 24 hours, you will be approved for unlimited private repositories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once the organization is created, you can add key team members to the “Owners” group, and then create another group for students and collaborators.&lt;/p&gt;

&lt;p&gt;Consider also that is not necessary for every collaborator to have write access to your repositories. My recommendation is to ask a more experienced team member to administer the central repository, ask the students to fork the repository under their user accounts (forks of private repositories are always private, free and don’t use any slot), and then &lt;a href=&quot;https://help.github.com/articles/using-pull-requests&quot;&gt;send a pull request&lt;/a&gt; to the central repository for the administrator to review, discuss and merge.&lt;/p&gt;

&lt;p&gt;See for example the organization account of the &lt;a href=&quot;https://github.com/dib-lab&quot;&gt;“The Lab for Data Intensive Biology” led by Dr. C. Titus Brown&lt;/a&gt; where they share code, documentation and papers. Open Science!!&lt;/p&gt;

&lt;p&gt;Other suggestions on the setup very welcome!&lt;/p&gt;</content><author><name></name></author><summary type="html">Github allows a research group to create their own webpage where they can host, share and develop their software using the git version control system and the powerful Github online issue-tracking interface.</summary></entry></feed>