<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://zonca.dev/feed.xml" rel="self" type="application/atom+xml" /><link href="https://zonca.dev/" rel="alternate" type="text/html" /><updated>2020-03-09T14:00:43-05:00</updated><id>https://zonca.dev/feed.xml</id><title type="html">Andrea Zonca</title><subtitle>Tutorials and blog posts by Andrea Zonca: Python, Jupyter, Kubernetes</subtitle><entry><title type="html">Migrate from Pelican to Fastpages</title><link href="https://zonca.dev/2020/03/pelican-to-fastpages.html" rel="alternate" type="text/html" title="Migrate from Pelican to Fastpages" /><published>2020-03-09T00:00:00-05:00</published><updated>2020-03-09T00:00:00-05:00</updated><id>https://zonca.dev/2020/03/pelican-to-fastpages</id><content type="html" xml:base="https://zonca.dev/2020/03/pelican-to-fastpages.html">&lt;p&gt;I have been using the Pelican static website generator for a few years,
hosting the content on Github, automatically build on push via Travis-CI
and deploy on Github pages to &lt;code class=&quot;highlighter-rouge&quot;&gt;zonca.github.io&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I am a heavy Jupyter Notebook user so once I saw the announcement of &lt;a href=&quot;https://fastpages.fast.ai/&quot;&gt;Fastpages&lt;/a&gt;
I decided it was time to switch.
I loved the idea of having Jupyter Notebooks built-in and not added via plugins,
also great idea to use Github actions.&lt;/p&gt;

&lt;p&gt;Only issue I found was that you cannot setup Fastpages on &lt;code class=&quot;highlighter-rouge&quot;&gt;username.github.io&lt;/code&gt;,
so went for using a custom domain name instead.&lt;/p&gt;

&lt;h2 id=&quot;import-content&quot;&gt;Import content&lt;/h2&gt;

&lt;p&gt;I created a script, in Python of course, to modify the front matter of the markdown
posts from the Pelican formatting to Jekyll, see &lt;a href=&quot;https://gist.github.com/zonca/b4a5a44513854e1c8918743d219f5f34&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pelican_to_jekyll.py&lt;/code&gt;&lt;/a&gt;.
It also renames the files, because Jekyll expects a date at the beginning of filenames.&lt;/p&gt;

&lt;h2 id=&quot;setup-paginate&quot;&gt;Setup paginate&lt;/h2&gt;

&lt;p&gt;Currently Fastpages doesn’t support pagination for the homepage,
but &lt;a href=&quot;https://github.com/fastai/fastpages/issues/48#issuecomment-596608688&quot;&gt;implemented a workaround&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;redirect-from-the-old-github-pages-blog&quot;&gt;Redirect from the old Github Pages blog&lt;/h2&gt;

&lt;p&gt;I modified the permalinks of Fastpages so that I have the same URLs in the old and new websites,
just the domain changes.
Github pages does not support custom rewriting rules, so I modified the Pelican template
to put a custom redirection tag in each HTML header.&lt;/p&gt;

&lt;p&gt;In the Pelican template &lt;code class=&quot;highlighter-rouge&quot;&gt;article.html&lt;/code&gt;, in the &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;header&amp;gt;&lt;/code&gt; section I added:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=https://zonca.dev/{{ article.url }}&quot;&amp;gt;
&amp;lt;link rel=&quot;canonical&quot; href=&quot;https://zonca.dev/{{ article.url }}&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So that Pelican regenerated all the articles with their original address
and automatically redirects upon access.
The canonical link hopefully helps with SEO.&lt;/p&gt;

&lt;p&gt;Did the same with the &lt;code class=&quot;highlighter-rouge&quot;&gt;index.html&lt;/code&gt; template to redirect the homepage,
this depends on your template:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; URL=https://zonca.dev&quot;&amp;gt;
&amp;lt;link rel=&quot;canonical&quot; href=&quot;https://zonca.dev/&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;screenshots-of-the-old-blog&quot;&gt;Screenshots of the old blog&lt;/h2&gt;

&lt;p&gt;Yeah, for posterity, growing older I get more nostalgic.&lt;/p&gt;

&lt;p&gt;The homepage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/old_blog_homepage.png&quot; alt=&quot;Old blog homepage&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A section of an article page:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/old_blog_article_page.png&quot; alt=&quot;Old blog article page&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">I have been using the Pelican static website generator for a few years, hosting the content on Github, automatically build on push via Travis-CI and deploy on Github pages to zonca.github.io.</summary></entry><entry><title type="html">Deploy CVMFS on Kubernetes</title><link href="https://zonca.dev/2020/02/cvmfs-kubernetes.html" rel="alternate" type="text/html" title="Deploy CVMFS on Kubernetes" /><published>2020-02-26T13:00:00-06:00</published><updated>2020-02-26T13:00:00-06:00</updated><id>https://zonca.dev/2020/02/cvmfs-kubernetes</id><content type="html" xml:base="https://zonca.dev/2020/02/cvmfs-kubernetes.html">&lt;p&gt;&lt;a href=&quot;https://cvmfs.readthedocs.io/&quot;&gt;CVMFS&lt;/a&gt; is a software distribution service, it is used by High Energy Physics experiments at CERN
to synchronize software environments across the whole collaborations.&lt;/p&gt;

&lt;p&gt;In the context of a Kubernetes + JupyterHub deployment on Jetstream, for example &lt;a href=&quot;http://zonca.github.io/2019/06/kubernetes-jupyterhub-jetstream-magnum.html&quot;&gt;deployed using Magnum following my tutorial&lt;/a&gt;, it is useful to use CVMFS to make the software tools of a collaboration to all the users connected to JupyterHub, so that we can keep the base Docker image simpler and smaller.&lt;/p&gt;

&lt;h2 id=&quot;alternatives&quot;&gt;Alternatives&lt;/h2&gt;

&lt;p&gt;A already existing solution is &lt;a href=&quot;https://github.com/cernops/cvmfs-csi&quot;&gt;the CVMFS CSI driver&lt;/a&gt;, however it doesn’t have much documentation, so I haven’t tested it. It would be useful for larger deployments, but we are designing for a 5 (possibly up to 10) nodes Kubernetes cluster.&lt;/p&gt;

&lt;h2 id=&quot;architecture&quot;&gt;Architecture&lt;/h2&gt;

&lt;p&gt;We have a pod running in Kubernetes (running as a privileged Docker container) which runs the CVMFS client and caches locally
(on a dedicated Openstack volume) some pre-defined CVMFS repositories (at the moment we do not support automounting).&lt;/p&gt;

&lt;p&gt;Currently we are using the &lt;code class=&quot;highlighter-rouge&quot;&gt;DIRECT&lt;/code&gt; connection for the CVMFS client, due to having just a single client which accesses
a small amount of data. Using a proxy is required instead for heavier usage, and it could also be deployed inside Kubernetes.&lt;/p&gt;

&lt;p&gt;The same pod also runs a NFS server and exposes it internally into the Kubernetes cluster, over the local Jetstream network,
to any other pod which can use a NFS volume and mount it to the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; folder inside the container.
We also activate the CVMFS configuration options for NFS support, following the &lt;a href=&quot;https://cvmfs.readthedocs.io/en/stable/cpt-configure.html#nfs-server-mode&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;deployment&quot;&gt;Deployment&lt;/h2&gt;

&lt;p&gt;The repositories used in this deployment are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zonca/docker-cvmfs-client&quot;&gt;Github repository for the Docker image of the CVMFS client&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Docker Hub repositories where the 2 containers are built: &lt;a href=&quot;https://hub.docker.com/r/zonca/cvmfs-client&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cvmfs-client&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://hub.docker.com/r/zonca/cvmfs-client-nfs&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cvmfs-client-nfs&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/tree/master/cvmfs&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;jupyterhub-deploy-kubernetes-jetstream&lt;/code&gt;&lt;/a&gt; Github repositories with the Kubernetes configuration files&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;First we need to checkout the &lt;code class=&quot;highlighter-rouge&quot;&gt;jupyterhub-deploy-kubernetes-jetstream&lt;/code&gt; repository:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream.git
cd jupyterhub-deploy-kubernetes-jetstream/cvmfs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then configure the CVMFS pod with the required repositories, see the &lt;code class=&quot;highlighter-rouge&quot;&gt;CVMFS_REPOSITORIES&lt;/code&gt; variable in &lt;a href=&quot;https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/blob/master/cvmfs/pod_cvmfs_nfs.yaml&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pod_cvmfs_nfs.yaml&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then deploy the pod with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f pod_cvmfs_nfs.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This creates 2 Openstack volumes, a 20 GB volume for the CVMFS cache, and a 1 GB volume which is just necessary as the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; root folder of the NFS server.
It also creates the &lt;code class=&quot;highlighter-rouge&quot;&gt;nfs-service&lt;/code&gt; Service, with a fixed IP, so that we can use it in the pod using this.&lt;/p&gt;

&lt;p&gt;Finally we can create a pod using mounting the folder via NFS:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f test_nfs_mount.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then get a terminal in the pod with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash ../terminal_pod.sh test-nfs-mount
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This creates a volume which mounts the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; folder shared with NFS, this automatically also shares also all the subfolders.&lt;/p&gt;

&lt;p&gt;Finally we can check the content of the &lt;code class=&quot;highlighter-rouge&quot;&gt;/cvmfs&lt;/code&gt; folder.&lt;/p&gt;</content><author><name></name></author><summary type="html">CVMFS is a software distribution service, it is used by High Energy Physics experiments at CERN to synchronize software environments across the whole collaborations.</summary></entry><entry><title type="html">Organize calendars for a large scientific collaboration</title><link href="https://zonca.dev/2019/12/organize-calendar-collaboration.html" rel="alternate" type="text/html" title="Organize calendars for a large scientific collaboration" /><published>2019-12-02T12:00:00-06:00</published><updated>2019-12-02T12:00:00-06:00</updated><id>https://zonca.dev/2019/12/organize-calendar-collaboration</id><content type="html" xml:base="https://zonca.dev/2019/12/organize-calendar-collaboration.html">&lt;p&gt;Many scientific collaborations have a central calendar, often hosted on Google Calendar,
to coordinate Teleconferences, meetings and events across timezones.&lt;/p&gt;

&lt;h3 id=&quot;the-issue&quot;&gt;The issue&lt;/h3&gt;

&lt;p&gt;Most users are only interested in a small subset of the events, however Google Calendar
does not allow them to subscribe to single events. The central calendar admin could invite
each person to events, but that requires lots of work.&lt;/p&gt;

&lt;p&gt;So, users either subscribe to the whole calendar, but then have a huge clutter of un-interesting events,
or copy just a subset of the events to their calendars, but loose track of any rescheduling of the
original event.&lt;/p&gt;

&lt;h3 id=&quot;proposed-solution&quot;&gt;Proposed solution&lt;/h3&gt;

&lt;p&gt;I recommend to split the events across multiple calendars, for example one for each working group,
or any other categorization where most users would be interested in all events in a calendar.
And possibly a “General” category with events that should interest the whole collaboration.&lt;/p&gt;

&lt;p&gt;Still, we can embed all of the calendars in a single webpage, see an example below where 2 calendars (Monday and Tuesday telecon calendars) are visualized together, &lt;a href=&quot;https://support.google.com/calendar/answer/41207?hl=en&quot;&gt;see the Google Calendar documentation&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;https://calendar.google.com/calendar/embed?height=600&amp;amp;wkst=1&amp;amp;bgcolor=%23ffffff&amp;amp;ctz=America%2FLos_Angeles&amp;amp;src=dTI2dnBkNnZvcm1qNHVucnVtajMzZzdwcGNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&amp;amp;src=c2FwazM1OTVmcHRiZHVtOWdqZnJwdWxkbnNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&amp;amp;color=%23DD4477&amp;amp;color=%236633CC&quot; style=&quot;border-width:0&quot; width=&quot;800&quot; height=&quot;600&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Users can click on the bottom “Add to Google Calendar” button and subscribe to a subset or all the calendars.
See the screenshot below, &lt;img src=&quot;/images/add_google_calendar.png&quot; alt=&quot;screenshot of add to Google Calendar&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;As an additional benefit, we can compartimentalize permissions more easily, e.g. leads of a working group
get writing access only to their relevant calendar/calendars.&lt;/p&gt;</content><author><name></name></author><summary type="html">Many scientific collaborations have a central calendar, often hosted on Google Calendar, to coordinate Teleconferences, meetings and events across timezones.</summary></entry><entry><title type="html">Simulate users on JupyterHub</title><link href="https://zonca.dev/2019/10/loadtest-jupyterhub.html" rel="alternate" type="text/html" title="Simulate users on JupyterHub" /><published>2019-10-30T12:00:00-05:00</published><updated>2019-10-30T12:00:00-05:00</updated><id>https://zonca.dev/2019/10/loadtest-jupyterhub</id><content type="html" xml:base="https://zonca.dev/2019/10/loadtest-jupyterhub.html">&lt;p&gt;I currently have 2 different strategies to deploy JupyterHub on top of Kubernetes on Jetstream:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using &lt;a href=&quot;https://zonca.github.io/2019/02/kubernetes-jupyterhub-jetstream-kubespray.html&quot;&gt;Kubespray&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Using &lt;a href=&quot;http://zonca.github.io/2019/06/kubernetes-jupyterhub-jetstream-magnum.html&quot;&gt;Magnum&lt;/a&gt;, which also supports the &lt;a href=&quot;http://zonca.github.io/2019/09/kubernetes-jetstream-autoscaler.html&quot;&gt;Cluster Autoscaler&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this tutorial I’ll show how to use Yuvi Pandas’ &lt;a href=&quot;https://github.com/yuvipanda/hubtraf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt;&lt;/a&gt; to simulate load on JupyterHub, i.e. programmatically generate a predefined number of users connecting and executing notebooks on the system.&lt;/p&gt;

&lt;p&gt;This is especially useful to test the Cluster Autoscaler.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; assumes you are using the Dummy authenticator, which is the default installed by the &lt;code class=&quot;highlighter-rouge&quot;&gt;zero-to-jupyterhub&lt;/code&gt; helm chart. If you have configured another authenticator, temporarily disable it for testing purposes.&lt;/p&gt;

&lt;p&gt;First go through the &lt;a href=&quot;https://github.com/yuvipanda/hubtraf/blob/master/docs/index.rst#jupyterhub-traffic-simulator&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; documentation&lt;/a&gt; to understand its functionalities.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; also has a Helm recipe to run it within Kubernetes, but the simpler way is to test from your laptop, follow the [documentation of &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt;] to install the package and then run:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hubtraf http://js-xxx-yyy.jetstream-cloud.org 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To simulate 2 users connecting to the system, you can then check with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods -n jhub
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That the pods are being created successfully and check the logs on the command line from &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; which explains what it is doing and tracks the time every operation takes, so it is useful to debug any delay in providing resources to users.&lt;/p&gt;

&lt;p&gt;Consider that volumes created by JupyterHub for the test users will remain in Kubernetes and in Openstack, therefore if you would like to use the same deployment for production, remember to cleanup the Kubernetes &lt;code class=&quot;highlighter-rouge&quot;&gt;PersistentVolume&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;PersistentVolumeClaim&lt;/code&gt; resources.&lt;/p&gt;

&lt;p&gt;Now we can test scalability of the deployment with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    hubtraf http://js-xxx-yyy.jetstream-cloud.org 100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Make sure you have asked the XSEDE support to increase the maximum number of volumes in Openstack in your allocation that by default is only 10. Otherwise edit &lt;code class=&quot;highlighter-rouge&quot;&gt;config_standard_storage.yaml&lt;/code&gt; and set:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;singleuser:
  storage:
    type: none
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;test-the-cluster-autoscaler&quot;&gt;Test the Cluster Autoscaler&lt;/h2&gt;

&lt;p&gt;If you followed the tutorial to deploy the Cluster Autoscaler on Magnum, you can launch &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; to create a large number of pods, then check that some pods are “Running” and the ones that do not fit in the current nodes are “Pending”:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods -n jhub
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then check in the logs of the autoscaler that it detects that those pods are pending and requests additional nodes.
For example:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; kubectl logs &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system cluster-autoscaler-hhhhhhh-uuuuuuu
I1031 00:48:39.807384       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2
I1031 00:48:41.583449       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&amp;gt;2
I1031 00:49:14.141351       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After 4 or 5 minutes the new node should be available and should show up in:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And we can check that some user pods are now running on the new node:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods -n jhub -o wide
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In my case the Autoscaler actually requested a 3rd node to accomodate all the users pods:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I1031 00:48:39.807384       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2
I1031 00:48:41.583449       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&amp;gt;2
I1031 00:49:14.141351       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
I1031 00:52:51.308054       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
I1031 00:53:01.315179       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 3
I1031 00:53:02.996583       1 magnum_nodegroup.go:101] Increasing size by 1, 2-&amp;gt;3
I1031 00:53:35.607158       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
I1031 00:56:41.834151       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Moreover Cluster Autoscaler also provides useful information in the status of each “Pending” node. For example if it detects that it is useless to create a new node because the node is “Pending” for some other reason (e.g. volume quota reached), this infomation will be accessible using:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl describe node -n jhub jupyter-xxxxxxx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When the simulated users disconnect, &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt; has a default of about 5 minutes, the autoscaler waits for the configured amount of minutes, by default it is 10 minutes, in my deployment it is 1 minute to simplify testing, see the &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-deployment-master.yaml&lt;/code&gt; file.
After this delay, the autoscaler scales down the size of the cluster, it is a 2 step process, it first terminates the Openstack Virtual machine and then adjusts the size of the Magnum cluster (&lt;code class=&quot;highlighter-rouge&quot;&gt;node_count&lt;/code&gt;), you can monitor the process using &lt;code class=&quot;highlighter-rouge&quot;&gt;openstack server list&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;openstack coe cluster list&lt;/code&gt;, and the log of the autoscaler:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I1101 06:31:10.223660       1 scale_down.go:882] Scale-down: removing empty node k8s-e2iw7axmhym7-minion-1 
I1101 06:31:16.081223       1 magnum_manager_heat.go:276] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;stack UPDATE_IN_PROGRESS status
I1101 06:32:17.061860       1 magnum_manager_heat.go:276] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;stack UPDATE_COMPLETE status
I1101 06:32:49.826439       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
I1101 06:33:21.588022       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;Thanks Yuvi Panda for providing &lt;code class=&quot;highlighter-rouge&quot;&gt;hubtraf&lt;/code&gt;, thanks Julien Chastang for testing my deployments.&lt;/p&gt;</content><author><name></name></author><summary type="html">I currently have 2 different strategies to deploy JupyterHub on top of Kubernetes on Jetstream:</summary></entry><entry><title type="html">Execute Jupyter Notebooks not interactively</title><link href="https://zonca.dev/2019/09/batch-notebook-execution.html" rel="alternate" type="text/html" title="Execute Jupyter Notebooks not interactively" /><published>2019-09-23T12:00:00-05:00</published><updated>2019-09-23T12:00:00-05:00</updated><id>https://zonca.dev/2019/09/batch-notebook-execution</id><content type="html" xml:base="https://zonca.dev/2019/09/batch-notebook-execution.html">&lt;p&gt;Over the years, I have explored how to scale up easily computation through
Jupyter Notebooks by executing them not-interactively, possibily parametrized
and remotely. This is mostly for reference.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/zonca/nbsubmit&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nbsubmit&lt;/code&gt;&lt;/a&gt; is a Python package which has Python API to send a local notebook for execution on a remote SLURM cluster, for example Comet, see &lt;a href=&quot;https://github.com/zonca/nbsubmit/blob/master/example/multiple_jobs/submit_multiple_jobs.ipynb&quot;&gt;an example&lt;/a&gt;. This project is not maintained right now.&lt;/li&gt;
  &lt;li&gt;Back in 2017 I tested submitting notebooks to Open Science Grid, see &lt;a href=&quot;https://github.com/zonca/batch-notebooks-condor&quot;&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;batch-notebooks-condor&lt;/code&gt; repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Back in 2016 I created scripts to template a Jupyter Notebook and launch SLURM jobs, see &lt;a href=&quot;https://github.com/sdsc/sdsc-summer-institute-2016/blob/master/hpc3_python_hpc/slurm.shared.template&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slurm.shared.template&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/sdsc/sdsc-summer-institute-2016/blob/master/hpc3_python_hpc/runipyloop.sh&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;runipyloop.sh&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Over the years, I have explored how to scale up easily computation through Jupyter Notebooks by executing them not-interactively, possibily parametrized and remotely. This is mostly for reference.</summary></entry><entry><title type="html">Deploy Cluster Autoscaler for Kubernetes on Jetstream</title><link href="https://zonca.dev/2019/09/kubernetes-jetstream-autoscaler.html" rel="alternate" type="text/html" title="Deploy Cluster Autoscaler for Kubernetes on Jetstream" /><published>2019-09-12T12:00:00-05:00</published><updated>2019-09-12T12:00:00-05:00</updated><id>https://zonca.dev/2019/09/kubernetes-jetstream-autoscaler</id><content type="html" xml:base="https://zonca.dev/2019/09/kubernetes-jetstream-autoscaler.html">&lt;p&gt;The &lt;a href=&quot;https://github.com/kubernetes/autoscaler&quot;&gt;Kubernetes Cluster Autoscaler&lt;/a&gt; is a service
that runs within a Kubernetes cluster and when there are not enough resources to accomodate
the pods that are queued to run, it contacts the API of the cloud provider to create
more Virtual Machines to join the Kubernetes Cluster.&lt;/p&gt;

&lt;p&gt;Initially the Cluster Autoscaler only supported commercial cloud provides, but back in
March 2019 &lt;a href=&quot;https://github.com/kubernetes/autoscaler/pull/1690&quot;&gt;a user contributed Openstack support based on Magnum&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First step you should have a Magnum-based deployment running on Jetstream,
see &lt;a href=&quot;https://zonca.github.io/2019/06/kubernetes-jupyterhub-jetstream-magnum.html&quot;&gt;my recent tutorial about that&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Therefore you should also have already a copy of the repository of all configuration
files checked out on your local machine that you are using to interact with the openstack API,
if not:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and enter the folder dedicated to the autoscaler:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd jupyterhub-deploy-kubernetes-jetstream/kubernetes_magnum/autoscaler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;setup-credentials&quot;&gt;Setup credentials&lt;/h2&gt;

&lt;p&gt;We first create the service account needed by the autoscaler to interact with the Kubernetes API:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cluster-autoscaler-svcaccount.yaml 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we need to provide all connection details for the autoscaler to interact with the Openstack API,
those are contained in the &lt;code class=&quot;highlighter-rouge&quot;&gt;cloud-config&lt;/code&gt; of our cluster available in the master node and setup
by Magnum.
Get the &lt;code class=&quot;highlighter-rouge&quot;&gt;IP&lt;/code&gt; of your master node from:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack server list
&lt;span class=&quot;nv&quot;&gt;IP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;xxx.xxx.xxx.xxx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now ssh into the master node and access the &lt;code class=&quot;highlighter-rouge&quot;&gt;cloud-config&lt;/code&gt; file:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh fedora@&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/kubernetes/cloud-config 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;now copy the &lt;code class=&quot;highlighter-rouge&quot;&gt;[Global]&lt;/code&gt; section at the end of &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-secret.yaml&lt;/code&gt; on the local machine.
Also remove the line of &lt;code class=&quot;highlighter-rouge&quot;&gt;ca-file&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cluster-autoscaler-secret.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;launch-the-autoscaler-deployment&quot;&gt;Launch the Autoscaler deployment&lt;/h2&gt;

&lt;p&gt;Create the Autoscaler deployment:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cluster-autoscaler-deployment-master.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Alternatively, I also added a version for a cluster where we are not deploying pods on master &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-deployment.yaml&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Check that the deployment is active:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system get pods
NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
cluster-autoscaler     1         1         1            0           10s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And check its logs:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system logs cluster-autoscaler-59f4cf4f4-4k4p2

I0905 05:29:21.589062       1 leaderelection.go:217] attempting to acquire leader lease  kube-system/cluster-autoscaler...
I0905 05:29:39.412449       1 leaderelection.go:227] successfully acquired lease kube-system/cluster-autoscaler
I0905 05:29:43.896557       1 magnum_manager_heat.go:293] For stack ID 17ab3ae7-1a81-43e6-98ec-b6ffd04f91d3, stack name is k8s-lu3bksbwsln3
I0905 05:29:44.146319       1 magnum_manager_heat.go:310] Found nested kube_minions stack: name k8s-lu3bksbwsln3-kube_minions-r4lhlv5xuwu3, ID d0590824-cc70-4da5-b9ff-8581d99c666b
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you redeploy the cluster and keep a older authentication, you’ll see “Authentication failed” in the logs of the autoscaler pod, you need to update the secret every time you redeploy the cluster.&lt;/p&gt;

&lt;h2 id=&quot;test-the-autoscaler&quot;&gt;Test the autoscaler&lt;/h2&gt;

&lt;p&gt;Now we need to produce a significant load on the cluster so that the autoscaler is triggered to request Openstack Magnum to create more Virtual Machines.&lt;/p&gt;

&lt;p&gt;We can create a deployment of the NGINX container (any other would work for this test):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create deployment autoscaler-demo &lt;span class=&quot;nt&quot;&gt;--image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then create a large number of replicas:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl scale deployment autoscaler-demo &lt;span class=&quot;nt&quot;&gt;--replicas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;300
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We are using 2 nodes with a large amount of memory and CPU, so they can accommodate more then 200 of those pods. The rest remains in the queue:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get deployment autoscaler-demo
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
autoscaler-demo   300       300       300          213         18m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And this triggers the autoscaler:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system logs cluster-autoscaler-59f4cf4f4-4k4p2

I0905 05:34:47.401149       1 scale_up.go:689] Scale-up: setting group DefaultNodeGroup size to 2
I0905 05:34:49.267280       1 magnum_nodegroup.go:101] Increasing size by 1, 1-&amp;gt;2
I0905 05:35:22.222387       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_IN_PROGRESS status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check also in the Openstack API:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack coe cluster list
+------+------+---------+------------+--------------+--------------------+
| uuid | name | keypair | node_count | master_count | status             |
+------+------+---------+------------+--------------+--------------------+
| 09fcf| k8s  | comet   |          2 |            1 | UPDATE_IN_PROGRESS |
+------+------+---------+------------+--------------+--------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It takes about 4 minutes for a new VM to boot, be configured by Magnum and join the Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Checking the logs again should show another line:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I0912 17:18:28.290987       1 magnum_nodegroup.go:67] Waited &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;cluster UPDATE_COMPLETE status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then you should have all 3 nodes available:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
NAME                        STATUS   ROLES    AGE   VERSION
k8s-6bawhy45wr5t-master-0   Ready    master   38m   v1.11.1
k8s-6bawhy45wr5t-minion-0   Ready    &amp;lt;none&amp;gt;   38m   v1.11.1
k8s-6bawhy45wr5t-minion-1   Ready    &amp;lt;none&amp;gt;   30m   v1.11.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and all 300 NGINX containers deployed:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get deployments
NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
autoscaler-demo   300       300       300          300         35m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also test scaling down by scaling back the number of NGINX containers to only a few and check in the logs
of the autoscaler that this process triggers the scale-down process.&lt;/p&gt;

&lt;p&gt;In &lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-autoscaler-deployment-master.yaml&lt;/code&gt; I have configured the scale down process to trigger just after 1 minute, to simplify testing. For production, better increase this to 10 minutes or more. Check the &lt;a href=&quot;https://github.com/zonca/autoscaler/blob/cluster-autoscaler-1.14-magnum/cluster-autoscaler/FAQ.md&quot;&gt;documentation of Cluster Autoscaler 1.14&lt;/a&gt; for all other available options.&lt;/p&gt;

&lt;h2 id=&quot;note-about-the-cluster-autoscaler-container&quot;&gt;Note about the Cluster Autoscaler container&lt;/h2&gt;

&lt;p&gt;The Magnum provider was added in Cluster Autoscaler 1.15, however this version is not compatible with Kubernetes 1.11 which is currently available on Jetstream. Therefore I have taken the development version of Cluster Autoscaler 1.14 and compiled it myself. I also noticed that the scale down process was not working due to incompatible IDs when the Cloud Provider tried to lookup the ID of a Minion in the Stack. I am now directly using the MachineID instead of going through these indices. This version is available in &lt;a href=&quot;https://github.com/zonca/autoscaler/tree/cluster-autoscaler-1.14-magnum&quot;&gt;my fork of &lt;code class=&quot;highlighter-rouge&quot;&gt;autoscaler&lt;/code&gt;&lt;/a&gt; and it is built into docker containers on the &lt;a href=&quot;https://cloud.docker.com/repository/docker/zonca/k8s-cluster-autoscaler-jetstream&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;zonca/k8s-cluster-autoscaler-jetstream&lt;/code&gt; repository on Docker Hub&lt;/a&gt;.
The image tags are the short version of the repository git commit hash.&lt;/p&gt;

&lt;p&gt;I build the container using the &lt;code class=&quot;highlighter-rouge&quot;&gt;run_gobuilder.sh&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;run_build_autoscaler_container.sh&lt;/code&gt; scripts included in the repository.&lt;/p&gt;

&lt;h2 id=&quot;note-about-images-used-by-magnum&quot;&gt;Note about images used by Magnum&lt;/h2&gt;

&lt;p&gt;I have tested this deployment using the &lt;code class=&quot;highlighter-rouge&quot;&gt;Fedora-Atomic-27-20180419&lt;/code&gt; image on Jetstream at Indiana University.
The Fedora Atomic 28 image had a long hang-up during boot and took more than 10 minutes to start and that caused timeout in the autoscaler and anyway it would have been too long for a user waiting to start a notebook.&lt;/p&gt;

&lt;p&gt;I also tried updating the Fedora Atomic 28 image with &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo atomic host upgrade&lt;/code&gt; and while this fixed the slow startup issue, it generated a broken Kubernetes installation, i.e. the Kubernetes services didn’t detect the master node as part of the cluster, &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get nodes&lt;/code&gt; only showed the minion.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Kubernetes Cluster Autoscaler is a service that runs within a Kubernetes cluster and when there are not enough resources to accomodate the pods that are queued to run, it contacts the API of the cloud provider to create more Virtual Machines to join the Kubernetes Cluster.</summary></entry><entry><title type="html">Create a Github account for your research group with free private repositories</title><link href="https://zonca.dev/2019/08/github-for-research-groups.html" rel="alternate" type="text/html" title="Create a Github account for your research group with free private repositories" /><published>2019-08-24T15:00:00-05:00</published><updated>2019-08-24T15:00:00-05:00</updated><id>https://zonca.dev/2019/08/github-for-research-groups</id><content type="html" xml:base="https://zonca.dev/2019/08/github-for-research-groups.html">&lt;p&gt;&lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt; allows a research group to create their own webpage where they can host, share and develop their software using the &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; version control system and the powerful Github online issue-tracking interface.&lt;/p&gt;

&lt;p&gt;Github offers unlimited private and public repositories to research groups and classrooms.
Private repositories are useful for early stages of development or if it is necessary to keep software secret before publication, at publication they can easily switched to public repositories and free up their slot.&lt;/p&gt;

&lt;p&gt;They also provide free data packs for &lt;a href=&quot;https://git-lfs.github.com/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt;(Large File Support)&lt;/a&gt; which is useful to store large amount of binary data together with your software in the same repository, without actually committing the files into &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; but using a support server. Just go into “Settings” for your organization and under “Billing” add data packs, you will notice that the cost is $0.&lt;/p&gt;

&lt;p&gt;Here the steps to set this up:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a user account on Github and choose the free plan, use your &lt;code class=&quot;highlighter-rouge&quot;&gt;.edu&lt;/code&gt; email address&lt;/li&gt;
  &lt;li&gt;Create an organization account for your research group&lt;/li&gt;
  &lt;li&gt;Go to &lt;a href=&quot;https://education.github.com/&quot;&gt;https://education.github.com/&lt;/a&gt; and click on “Get benefits”&lt;/li&gt;
  &lt;li&gt;Choose what is your position, e.g. Researcher and select you want a discount for an organization&lt;/li&gt;
  &lt;li&gt;Choose the organization you created earlier and confirm that it is a “Research group”&lt;/li&gt;
  &lt;li&gt;Add details about your Research group&lt;/li&gt;
  &lt;li&gt;Finally you need to upload a picture of your University ID card and write how you plan on using the repositories&lt;/li&gt;
  &lt;li&gt;Within a week at most, but generally in less than 24 hours, you will be approved for unlimited private repositories.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once the organization is created, you can add key team members to the “Owners” group, and then create another group for students and collaborators.&lt;/p&gt;

&lt;p&gt;Consider also that is not necessary for every collaborator to have write access to your repositories. My recommendation is to ask a more experienced team member to administer the central repository, ask the students to fork the repository under their user accounts (forks of private repositories are always private, free and don’t use any slot), and then &lt;a href=&quot;https://help.github.com/articles/using-pull-requests&quot;&gt;send a pull request&lt;/a&gt; to the central repository for the administrator to review, discuss and merge.&lt;/p&gt;

&lt;p&gt;See for example the organization account of the &lt;a href=&quot;https://github.com/dib-lab&quot;&gt;“The Lab for Data Intensive Biology” led by Dr. C. Titus Brown&lt;/a&gt; where they share code, documentation and papers. Open Science!!&lt;/p&gt;

&lt;p&gt;Other suggestions on the setup very welcome!&lt;/p&gt;</content><author><name></name></author><summary type="html">Github allows a research group to create their own webpage where they can host, share and develop their software using the git version control system and the powerful Github online issue-tracking interface.</summary></entry><entry><title type="html">Ship large files with Python packages</title><link href="https://zonca.dev/2019/08/large-files-python-packages.html" rel="alternate" type="text/html" title="Ship large files with Python packages" /><published>2019-08-21T18:00:00-05:00</published><updated>2019-08-21T18:00:00-05:00</updated><id>https://zonca.dev/2019/08/large-files-python-packages</id><content type="html" xml:base="https://zonca.dev/2019/08/large-files-python-packages.html">&lt;p&gt;It is often useful to ship large data files together with a Python package,
a couple of scenarios are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;data necessary to the functionality provided by the package, for example images, any binary or large text dataset, they could be either required just for a subset of the functionality of the package or for all of it&lt;/li&gt;
  &lt;li&gt;data necessary for unit or integration testing, both example inputs and expected outputs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If data are collectively less than 2 GB compressed and do not change very often, a simple and a bit hacky solution is to use GitHub release assets. For each packaged release on GitHub it is possible to attach one or more assets smaller than 2 GB. You can then attach data to each release, the downside is that users need to make sure to use the correct dataset for the release they are using and the first time they use the software the need to install the Python package and also download the dataset and install it in the right folder. See &lt;a href=&quot;https://gist.github.com/zonca/52857f2425942725fb74595c4f8600e9&quot;&gt;an example script to upload from the command line&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If data files are individually less than 10 MB and collectively less than 100 MB you can directly add them into the Python package. This is the easiest and most convenient option, for example the &lt;a href=&quot;https://github.com/astropy/package-template&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;astropy package template&lt;/code&gt;&lt;/a&gt; automatically adds to the package any file inside the &lt;code class=&quot;highlighter-rouge&quot;&gt;packagename/data&lt;/code&gt; folder.&lt;/p&gt;

&lt;p&gt;For larger datasets I recommend to host the files externally and use the &lt;a href=&quot;http://docs.astropy.org/en/stable/utils/#module-astropy.utils.data&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;astropy.utils.data&lt;/code&gt; module&lt;/a&gt;.
This module automates the process of retrieving a file from a remote server and caching it locally (in the users home folder), next time the user needs it, it is automatically retrieved from the cache:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;dataurl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://my-web-server.ucsd.edu/test-data/&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataurl&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataurl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;remote_timeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;local_file_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_pkg_data_filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;myfile.jpg)&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we need to host there files publicly, I have a few options.&lt;/p&gt;

&lt;h3 id=&quot;host-on-a-dedicated-github-repository&quot;&gt;Host on a dedicated GitHub repository&lt;/h3&gt;

&lt;p&gt;If files are individually less than 100MB and collectively a few GB, you can create a dedicated repository on GitHub and push there your files.
Then &lt;a href=&quot;https://help.github.com/en/articles/what-is-github-pages&quot;&gt;activate GitHub Pages&lt;/a&gt; so that those files are published at &lt;code class=&quot;highlighter-rouge&quot;&gt;https://your-organization.github.io/your-repository/&lt;/code&gt;.
Then use this URL as &lt;code class=&quot;highlighter-rouge&quot;&gt;dataurl&lt;/code&gt; in the above script.&lt;/p&gt;

&lt;h3 id=&quot;host-on-a-supercomputer-or-own-server&quot;&gt;Host on a Supercomputer or own server&lt;/h3&gt;

&lt;p&gt;Some Supercomputers offer the feature of providing public web access from specific folders, for example NERSC allows user to publish web-pages publicly, see &lt;a href=&quot;https://www.nersc.gov/users/computational-systems/pdsf/software-and-tools/hosting-webpages/&quot;&gt;their documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is very useful for huge datasets because you can automatically detect if the package is being run at NERSC and then automatically access the files with their path instead of downloading them.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_data_from_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Retrieves input templates from remote server,
    in case data is available in one of the PREDEFINED_DATA_FOLDERS defined above,
    e.g. at NERSC, those are directly returned.&quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;folder&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PREDEFINED_DATA_FOLDERS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;full_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;full_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;warnings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;warn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Access data from {full_path}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full_path&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dataurl&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATAURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;remote_timeout&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;warnings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;warn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Retrieve data for {filename} (if not cached already)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;map_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_pkg_data_filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;show_progress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;map_out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similar setup can be achieved on a GNU/Linux server, for example a powerful machine used by all members of a scientific team, where a folder is dedicated to host these data and is also published online with Apache or NGINX.&lt;/p&gt;

&lt;p&gt;The main downside of this approach is that there is no built-in version control. One possibility is to enforce a policy where no files are ever overwritten and version control is automatically achieved with filenames. Otherwise, use &lt;a href=&quot;https://git-lfs.github.com/&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;git lfs&lt;/code&gt;&lt;/a&gt; in that folder to track any change in a dedicated local &lt;code class=&quot;highlighter-rouge&quot;&gt;git&lt;/code&gt; repository, e.g.:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
git init
git lfs track &lt;span class=&quot;s2&quot;&gt;&quot;*.fits&quot;&lt;/span&gt;
git add &lt;span class=&quot;s2&quot;&gt;&quot;*.fits&quot;&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;initial version of all FITS files&quot;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This method tracks the checksum of all the binary files and helps managing the history, even if only locally (make sure the folder is also regularly backed up). You could push it to GitHub, that would cost $5/month for each 50GB of storage.&lt;/p&gt;

&lt;h3 id=&quot;host-on-figshare&quot;&gt;Host on Figshare&lt;/h3&gt;

&lt;p&gt;You can upload files to Figshare using the browser and create a dataset which also comes with a DOI and a page where you can save metadata about this object.&lt;/p&gt;

&lt;p&gt;Once you have set the dataset public, you can find out the URL of the actual file, which is of the form &lt;code class=&quot;highlighter-rouge&quot;&gt;https://ndownloader.figshare.com/files/2432432432&lt;/code&gt;, therefore we can set &lt;code class=&quot;highlighter-rouge&quot;&gt;https://ndownloader.figshare.com/files/&lt;/code&gt; as the repository and use the integer defined in Figshare as filename. Using integers as filenames makes it a bit cryptic, but it has the great advantage that other people can do the uploading to Figshare and you can point to their files as easily as if the are yours. This is more convenient than alternatives where instead you need to give other people access to your file repository.&lt;/p&gt;

&lt;h3 id=&quot;host-on-amazon-s3-or-other-object-store&quot;&gt;Host on Amazon S3 or other object store&lt;/h3&gt;

&lt;p&gt;A public bucket on Amazon S3 or other object store provides cheap storage and built-in version control.
The cost currently is about $0.026/GB/month.&lt;/p&gt;

&lt;p&gt;First login to the AWS console and create a new bucket, set it public by turning of “Block all public access” and under “Access Control List” set “List objects” to Yes for “Public access”.&lt;/p&gt;

&lt;p&gt;You could upload files with the browser, but for larger files command line is better.&lt;/p&gt;

&lt;p&gt;The files will be available at &lt;a href=&quot;https://bucket-name.s3-us-west-1.amazonaws.com/&quot;&gt;https://bucket-name.s3-us-west-1.amazonaws.com/&lt;/a&gt;, this changes based on the chosen region.&lt;/p&gt;

&lt;h4 id=&quot;advanced-upload-files-from-the-command-line&quot;&gt;(Advanced) Upload files from the command line&lt;/h4&gt;

&lt;p&gt;This is optional and requires some more familiarity with AWS.
Go back to the AWS console to the Identity and Access Management (IAM) section, then users, create, create a policy to give access only to 1 bucket (replace &lt;code class=&quot;highlighter-rouge&quot;&gt;bucket-name&lt;/code&gt;):&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2012-10-17&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Statement&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Sid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ListObjectsInBucket&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Effect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Action&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;s3:ListBucket&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:s3:::bucket-name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Sid&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;AllObjectActions&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Effect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Allow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Action&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;s3:*Object&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;s3:PutObjectAcl&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;Resource&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;arn:aws:s3:::bucket-name/*&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See the &lt;a href=&quot;https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_rw-bucket.html&quot;&gt;AWS documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Install &lt;code class=&quot;highlighter-rouge&quot;&gt;s3cmd&lt;/code&gt;, then run &lt;code class=&quot;highlighter-rouge&quot;&gt;s3cmd --configure&lt;/code&gt; to set it up and paste the Access and Secret keys, it will fail to test the configuration because it cannot list all the buckets, anyway choose to save the configuration.&lt;/p&gt;

&lt;p&gt;Test it:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    s3cmd &lt;span class=&quot;nb&quot;&gt;ls &lt;/span&gt;s3://bucket-name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then upload your files (reduced redundancy is cheaper):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    s3cmd put &lt;span class=&quot;nt&quot;&gt;--reduced-redundancy&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--acl-public&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.fits s3://bucket-name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">It is often useful to ship large data files together with a Python package, a couple of scenarios are:</summary></entry><entry><title type="html">Deploy Kubernetes and JupyterHub on Jetstream with Magnum</title><link href="https://zonca.dev/2019/06/kubernetes-jupyterhub-jetstream-magnum.html" rel="alternate" type="text/html" title="Deploy Kubernetes and JupyterHub on Jetstream with Magnum" /><published>2019-06-14T00:00:00-05:00</published><updated>2019-06-14T00:00:00-05:00</updated><id>https://zonca.dev/2019/06/kubernetes-jupyterhub-jetstream-magnum</id><content type="html" xml:base="https://zonca.dev/2019/06/kubernetes-jupyterhub-jetstream-magnum.html">&lt;p&gt;This tutorial deploys Kubernetes on Jetstream with Magnum and then
JupyterHub on top of that using &lt;a href=&quot;https://zero-to-jupyterhub.readthedocs.io/&quot;&gt;zero-to-jupyterhub&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In my &lt;a href=&quot;https://zonca.github.io/2019/02/kubernetes-jupyterhub-jetstream-kubespray.html&quot;&gt;previous tutorials&lt;/a&gt; I deployed Kubernetes using Kubespray. The main driver to using Magnum is that there is support for autoscaling, i.e. create and destroy Openstack instances based on the load on JupyterHub. I haven’t tested that yet, though, that will come in a following tutorial.&lt;/p&gt;

&lt;p&gt;Magnum is a technology built into Openstack to deploy Container Orchestration engines based on templates. The main difference with kubespray is that is way less configurable, the user does not have access to modify those templates but has just a number of parameters to set. Instead Kubespray is based on &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible&lt;/code&gt; and the user has full control of how the system is setup, it also supports having more High Availability features like multiple master nodes.
On the other hand, the &lt;code class=&quot;highlighter-rouge&quot;&gt;ansible&lt;/code&gt; recipe takes a very long time to run, ~30 min, while Magnum creates a cluster in about 10 minutes.&lt;/p&gt;

&lt;h2 id=&quot;setup-access-to-the-jetstream-api&quot;&gt;Setup access to the Jetstream API&lt;/h2&gt;

&lt;p&gt;First install the OpenStack client, please use these exact versions, also please run at Indiana, which currently has the Rocky release of Openstack, the TACC deployment has an older release of Openstack.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install python-openstackclient==3.16 python-magnumclient==2.10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Load your API credentials from &lt;code class=&quot;highlighter-rouge&quot;&gt;openrc.sh&lt;/code&gt;, check &lt;a href=&quot;https://iujetstream.atlassian.net/wiki/spaces/JWT/pages/39682064/Setting+up+openrc.sh&quot;&gt;documentation of the Jetstream wiki for details&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You need to have a keypair uploaded to Openstack, this just needs to be done once per account. See &lt;a href=&quot;https://iujetstream.atlassian.net/wiki/spaces/JWT/pages/35913730/OpenStack+command+line&quot;&gt;the Jetstream documentation&lt;/a&gt; under the section “Upload SSH key - do this once”.&lt;/p&gt;

&lt;h2 id=&quot;create-the-cluster-with-magnum&quot;&gt;Create the cluster with Magnum&lt;/h2&gt;

&lt;p&gt;As usual, checkout the repository with all the configuration files on the machine you will use the Jetstream API from, typically your laptop.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream
cd jupyterhub-deploy-kubernetes-jetstream
cd kubernetes_magnum
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we are ready to use Magnum to first create a cluster template and then the actual cluster, edit first &lt;code class=&quot;highlighter-rouge&quot;&gt;create_cluster.sh&lt;/code&gt; and set the parameters of the cluster on the top. Also make sure to set the keypair name.
Finally run:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash create_network.sh
bash create_template.sh
bash create_cluster.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have setup a test cluster with only 1 master node and 1 normal node but you can modify that later.&lt;/p&gt;

&lt;p&gt;Check the status of your cluster, after about 10 minutes, it should be in state &lt;code class=&quot;highlighter-rouge&quot;&gt;CREATE_COMPLETE&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack coe cluster show k8s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;configure-kubectl-locally&quot;&gt;Configure kubectl locally&lt;/h3&gt;

&lt;p&gt;Install the &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; client locally, first check the version of the master node:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack server list # find the floating public IP of the master node (starts with 149_
IP=149.xxx.xxx.xxx
ssh fedora@$IP
kubectl version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now install the same version following the &lt;a href=&quot;https://kubernetes.io/docs/tasks/tools/install-kubectl/&quot;&gt;Kubernetes documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now configure &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; on your laptop to connect to the Kubernetes cluster created with Magnum:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir kubectl_secret
cd kubectl_secret
openstack coe cluster config k8s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This downloads a configuration file and the required certificates.&lt;/p&gt;

&lt;p&gt;and returns  &lt;code class=&quot;highlighter-rouge&quot;&gt;export KUBECONFIG=/absolute/path/to/config&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;See also the &lt;code class=&quot;highlighter-rouge&quot;&gt;update_kubectl_secret.sh&lt;/code&gt; script to automate this step, but it requires to already have setup the environment variable.&lt;/p&gt;

&lt;p&gt;execute that and then:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;configure-storage&quot;&gt;Configure storage&lt;/h2&gt;

&lt;p&gt;Magnum configures a provider that knows how to create Kubernetes volumes using Openstack Cinder,
but does not configure a &lt;code class=&quot;highlighter-rouge&quot;&gt;storageclass&lt;/code&gt;, we can do that with:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f storageclass.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can test this by creating a Persistent Volume Claim:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f persistent_volume_claim.yaml

kubectl describe pv

kubectl describe pvc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Name:            pvc-e8b93455-898b-11e9-a37c-fa163efb4609
Labels:          failure-domain.beta.kubernetes.io/zone=nova
Annotations:     kubernetes.io/createdby: cinder-dynamic-provisioner
                 pv.kubernetes.io/bound-by-controller: yes
                 pv.kubernetes.io/provisioned-by: kubernetes.io/cinder
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    standard
Status:          Bound
Claim:           default/pvc-test
Reclaim Policy:  Delete
Access Modes:    RWO
Capacity:        5Gi
Node Affinity:   &amp;lt;none&amp;gt;
Message:         
Source:
    Type:       Cinder (a Persistent Disk resource in OpenStack)
    VolumeID:   2795724b-ef11-4053-9922-d854107c731f
    FSType:     
    ReadOnly:   false
    SecretRef:  nil
Events:         &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can also test creating an actual pod with a persistent volume and check
that the volume is successfully mounted and the pod started:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f ../alpine-persistent-volume.yaml
kubectl describe pod alpine
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;note-about-availability-zones&quot;&gt;Note about availability zones&lt;/h3&gt;

&lt;p&gt;By default Openstack servers and Openstack volumes are created in different availability zones. This created an issue with the default Magnum templates because we need to modify the Kubernetes scheduler policy to allow this. Kubespray does this by default, so I created a &lt;a href=&quot;https://github.com/zonca/magnum/pull/1&quot;&gt;fix to be applied to the Jetstream Magnum templates&lt;/a&gt;, this needs to be re-applied after every Openstack upgrade.&lt;/p&gt;

&lt;h2 id=&quot;install-helm&quot;&gt;Install Helm&lt;/h2&gt;

&lt;p&gt;The Kubernetes deployment from Magnum is not as complete as the one out of Kubespray, we need
to setup &lt;code class=&quot;highlighter-rouge&quot;&gt;helm&lt;/code&gt; and the NGINX ingress ourselves. We would also need to setup a system to automatically
deploy HTTPS certificates, I’ll add this later on.&lt;/p&gt;

&lt;p&gt;First &lt;a href=&quot;https://helm.sh/docs/using_helm/#installing-helm&quot;&gt;install the Helm client on your laptop&lt;/a&gt;, make
sure you have configured &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; correctly.&lt;/p&gt;

&lt;p&gt;Then we need to create a service account to give enough privilege to Helm to reconfigure the cluster:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f tiller_service_account.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we can create the &lt;code class=&quot;highlighter-rouge&quot;&gt;tiller&lt;/code&gt; pod inside Kubernetes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm init --service-account tiller --wait --history-max 200
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get pods --all-namespaces
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE
kube-system   coredns-78df4bf8ff-f2xvs                   1/1     Running   0          2d
kube-system   coredns-78df4bf8ff-pnj7g                   1/1     Running   0          2d
kube-system   heapster-74f98f6489-xsw52                  1/1     Running   0          2d
kube-system   kube-dns-autoscaler-986c49747-2m64g        1/1     Running   0          2d
kube-system   kubernetes-dashboard-54cb7b5997-c2vwx      1/1     Running   0          2d
kube-system   openstack-cloud-controller-manager-tf5mc   1/1     Running   3          2d
kube-system   tiller-deploy-6b5cd64488-4fkff             1/1     Running   0          20s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And check that all the versions agree:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;helm version
Client: &amp;amp;version.Version{SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;}
Server: &amp;amp;version.Version{SemVer:&quot;v2.11.0&quot;, GitCommit:&quot;2e55dbe1fdb5fdb96b75ff144a339489417b146b&quot;, GitTreeState:&quot;clean&quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;setup-nginx-ingress&quot;&gt;Setup NGINX ingress&lt;/h2&gt;

&lt;p&gt;We need to have the NGINX web server to act as front-end to the services running inside the Kubernetes cluster.&lt;/p&gt;

&lt;h3 id=&quot;open-http-and-https-ports&quot;&gt;Open HTTP and HTTPS ports&lt;/h3&gt;

&lt;p&gt;First we need to open the HTTP and HTTPS ports on the master node, you can either connect to the Horizon interface,
create new rule named &lt;code class=&quot;highlighter-rouge&quot;&gt;http_https&lt;/code&gt;, then add 2 rules, in the Rule drop down choose HTTP and HTTPS; or from the command line:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack security group create http_https
openstack security group rule create --ingress --protocol tcp --dst-port 80 http_https 
openstack security group rule create --ingress --protocol tcp --dst-port 443 http_https 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you can find the name of the master node in &lt;code class=&quot;highlighter-rouge&quot;&gt;openstack server list&lt;/code&gt; then add this security group to that instance:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openstack server add security group  k8s-xxxxxxxxxxxx-master-0 http_https
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;install-nginx-ingress-with-helm&quot;&gt;Install NGINX ingress with Helm&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash install_nginx_ingress.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note, the documentation says we should add this annotation to ingress with &lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl edit ingress -n jhub&lt;/code&gt;, but I found out it is not necessary:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;metadata:
  annotations:
    kubernetes.io/ingress.class: nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If this is correctly working, you should be able to run &lt;code class=&quot;highlighter-rouge&quot;&gt;curl localhost&lt;/code&gt; from the master node and get a &lt;code class=&quot;highlighter-rouge&quot;&gt;Default backend: 404&lt;/code&gt; message.&lt;/p&gt;

&lt;h2 id=&quot;install-jupyterhub&quot;&gt;Install JupyterHub&lt;/h2&gt;

&lt;p&gt;Finally, we can go back to the root of the repository and install JupyterHub, first create the secrets file:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash create_secrets.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then edit &lt;code class=&quot;highlighter-rouge&quot;&gt;secrets.yaml&lt;/code&gt; and modify the hostname under &lt;code class=&quot;highlighter-rouge&quot;&gt;hosts&lt;/code&gt; to display the hostname of your master Jetstream instance, i.e. if your instance public floating IP is &lt;code class=&quot;highlighter-rouge&quot;&gt;aaa.bbb.xxx.yyy&lt;/code&gt;, the hostname should be &lt;code class=&quot;highlighter-rouge&quot;&gt;js-xxx-yyy.jetstream-cloud.org&lt;/code&gt; (without &lt;code class=&quot;highlighter-rouge&quot;&gt;http://&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;You should also check that connecting with your browser to &lt;code class=&quot;highlighter-rouge&quot;&gt;js-xxx-yyy.jetstream-cloud.org&lt;/code&gt; shows &lt;code class=&quot;highlighter-rouge&quot;&gt;default backend - 404&lt;/code&gt;, this means NGINX is also reachable from the internet, i.e. the web port is open on the master node.&lt;/p&gt;

&lt;p&gt;Finally:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash configure_helm_jupyterhub.sh
bash install_jhub.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Connect with your browser to &lt;code class=&quot;highlighter-rouge&quot;&gt;js-xxx-yyy.jetstream-cloud.org&lt;/code&gt; to check if it works.&lt;/p&gt;

&lt;h2 id=&quot;issues-and-feedback&quot;&gt;Issues and feedback&lt;/h2&gt;

&lt;p&gt;Please &lt;a href=&quot;https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/&quot;&gt;open an issue on the repository&lt;/a&gt; to report any issue or give feedback. Also you find out there there what I am working on next.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;Many thanks to Jeremy Fischer and Mike Lowe for solving all my tickets, this required a lot of work on their end to make it working.&lt;/p&gt;</content><author><name></name></author><summary type="html">This tutorial deploys Kubernetes on Jetstream with Magnum and then JupyterHub on top of that using zero-to-jupyterhub.</summary></entry><entry><title type="html">Webinar about distributed computing with Python</title><link href="https://zonca.dev/2019/05/webinar-python-hpc.html" rel="alternate" type="text/html" title="Webinar about distributed computing with Python" /><published>2019-05-30T15:00:00-05:00</published><updated>2019-05-30T15:00:00-05:00</updated><id>https://zonca.dev/2019/05/webinar-python-hpc</id><content type="html" xml:base="https://zonca.dev/2019/05/webinar-python-hpc.html">&lt;p&gt;Recording available of the webinar I gave about “Distributed computing with Python”:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Threads vs Processes, GIL&lt;/li&gt;
  &lt;li&gt;Just-In-Time compilation with Numba&lt;/li&gt;
  &lt;li&gt;Processing data larger than memory with Dask&lt;/li&gt;
  &lt;li&gt;Distributed computing with Dask&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Live demo on my favorite Supercomputer Comet at the San Diego Supercomputer Center.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sdsc.edu/Events/training/webinars/distributed_parallel_computing_with_python_2019/recording/&quot;&gt;Webinar recording&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Notebooks: &lt;a href=&quot;https://github.com/zonca/python_hpc_tutorial&quot;&gt;https://github.com/zonca/python_hpc_tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Recording available of the webinar I gave about “Distributed computing with Python”:</summary></entry></feed>